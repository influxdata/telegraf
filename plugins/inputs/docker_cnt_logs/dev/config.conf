[global_tags]
  source = "docker_cnt_logs"
  process = "container"
  type = "logs"

[agent]
  interval = "1000ms"
  round_interval = false

  metric_batch_size = 1000
  metric_buffer_limit = 100000
  collection_jitter = "0s"
  flush_interval = "250ms"
  flush_jitter = "0s"
  precision = ""

  debug = false
  quiet = false
  logfile = ""

  hostname = ""
  omit_hostname = true

#[[inputs.internal]]

[[inputs.docker_cnt_logs]]
  interval = "500ms"

  # Docker Endpoint
  #  To use unix, set endpoint = "unix:///var/run/docker.sock" (/var/run/docker.sock is default mount path)
  #  To use TCP, set endpoint = "tcp://[ip]:[port]"
  #  To use environment variables (ie, docker-machine), set endpoint = "ENV"
  endpoint = "unix:///var/run/docker.sock"

  ## Optional TLS Config
  # tls_ca = "/etc/telegraf/ca.pem"
  # tls_cert = "/etc/telegraf/cert.pem"
  # tls_key = "/etc/telegraf/key.pem"

  ## Use TLS but skip chain & host verification
  # insecure_skip_verify = false

  #############################################################################
  # Log streaming settings

  # Set initial chunk size (length of []byte buffer to read from docker socket)
  # If not set, default value of 'defaultInitialChunkSize = 10000' will be used
  initial_chunk_size = 10000 # 10K symbols

  # Set max chunk size (length of []byte buffer to read from docker socket)
  # If not set, default value of 'defaultMaxChunkSize = 50000' will be used
  # buffer can grow in capacity adjusting to volume of data received from docker sock
  max_chunk_size = 50000 # 50K symbols



  # Offset flush interval. How often the offset pointer (see below) in the
  # log stream is flashed to file.Offset pointer represents the unix time stamp
  # for last message read from log stream (default - 3 sec)
  # offset_flush = "3s"

  # Offset storage path (mandatory)
  #offset_storage_path = "/var/run/collector_offset"
  offset_storage_path = "/Users/ilya.prudnikov/DevNS/src/github.com/influxdata/telegraf/plugins/inputs/docker_cnt_logs/collector_offset"

  # Shutdown telegraf if all log streaming containers stopped/killed, default - false
  # shutdown_when_eof = false
  shutdown_when_eof = true

  #Settings per container (specify as many sections as needed)

  [[inputs.docker_cnt_logs.container]]
    # Set container id (long or short from), or container name
    # to stream logs from, this attribute is mandatory
    id = "f469ccc5f37e"

    ## Override common settings
    ## input interval (specified or inherited from agent section)

    #interval = "500ms"

    ## Initial chunk size
    initial_chunk_size = 20000 # 2 Mb

    ## Max chunk size
    max_chunk_size = 100000 # 6 Mb

    #Set additional tags that will be tagged to the stream from the current container.
    #tags = [
    #    "tag1=value1",
    #    "tag2=value2"
    #]

  #[[inputs.docker_cnt_logs.container]]
  #  id = "009d82030745c9994e2f5c2280571e8b9f95681793a8f7073210759c74c1ea36"
  #  interval = "600ms"


  [inputs.docker_cnt_logs.tags]
    service = "collector"

# [[outputs.kafka]]
#   brokers = ["logs-01.test.env:9092","logs-02.test.env:9092","logs-03.test.env:9092"]
#   topic = "collector"
#   client_id = "collector"
#   version = ""
#   compression_codec = 0
#   required_acks = -1
#   max_retry = 3
#   max_message_bytes = 500000
#   insecure_skip_verify = false
#   data_format = "json"
#   json_timestamp_units = "1ms"

#   [outputs.kafka.topic_suffix]
#     method = "tags"
#     keys = ["type"]
#     separator = "."

# [[outputs.prometheus_client]]
#   listen = "0.0.0.0:65000"
#   path = "/metrics"
#   expiration_interval = "10s"

#[[processors.combine]]
#  format = "{{ index .tags \"source\"}}|{{index .tags \"process\"}}"
#  destType = "tag"
#  dest = "myNewTag"


[[outputs.file]]
  files = ["stdout"]
  data_format = "influx"